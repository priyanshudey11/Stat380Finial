---
title: "Final_Project"
author: 
  - "Ahsan Sultan"
  - "Priyanshu Dey" 
  - Janvi Ahuja" 
  - "Daniel Miller"
output: html_document
date: "2024-12-10"
---

1. [Part 1: Data Cleaning and Data Visualization – Complete without Generative AI](#part1)
2. [Part 2: Data Cleaning and Data Visualization – Complete with Generative AI](#part2)
3. [Part 3:Inference ](#part3)
4. [Part 4: Prediction](#part4)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
remove(list = ls())
#Loading in Essential Libraries
library(tidyverse)
library(ggplot2)
library(e1071)
library(caTools)
library(caret)
library(randomForest)
```
```{r}
#Loading in the Dataxdsw3
CODGames_p1_380 <- read.csv("CODMaps.csv") #589 rows, 27 columns
CODGames_p2_380 <- read.csv("CODGames_p2_380.csv") #250 rows, 27 columns
CODMaps <- read.csv("CODGames_p2_380.csv")
CODGameModes <- read.csv("CODGameModes.csv")
```

# Task 1 - Data Cleaning and Data Visualization   {#part1}
My thought process is that after reading through the Final Project PDF and going over the data csvs I realized that most of the CSVs need some cleansing to do. Since CODGames_p1_380 and CODGames_p2_380 are basically about a player each and their rankings in the online game, I think we should merge the dataframes to make it one more combined dataframe. I also realized that each of the two csvs have null values in the map and choices column so it would be important to remove them to keep all values in check. There are also trailing spaces as highlighted in the document so we would have to remove that also. I think it would be smart to eliminate rows that have missing choice values because they cant determine the winner. My thinking for the map winning criteria was to get unique maps from both map1 and map2 and then count the number of times they appear and then count wins where the map was chosen. Then I would count win from ties when map was map1.

```{r}
### I feel we should double check if we should use rbind or bind_rows
combined <- bind_rows(CODGames_p1_380, CODGames_p2_380)

combined <- combined %>%
  mutate(Map1 = trimws(Map1),
         Map2 = trimws(Map2),
         Choice = trimws(Choice)) %>%
  filter(!is.na(Map1) & !is.na(Map2) & !is.na(Choice)) 
```

### Calculating Map Choices Ratio/Stats
```{r}
calculate_win_stats <- function(data) {

    # Get unique maps
    maps <- data %>%
        select(Map1, Map2) %>%
        unlist() %>%
        unique() %>%
        .[!is.na(.)]
    
    # Initialize results dataframe
    results <- data.frame(
        Map = maps,
        Appearances = 0,
        Regular_Wins = 0,
        Tie_Wins = 0,
        Win_Rate = 0
    )
    
    for (map in maps) {
        # Calculate statistics using pipes
        stats <- data %>%
            summarize(
                appearances = sum(Map1 == map | Map2 == map, na.rm = TRUE),
                regular_wins = sum(Choice == map & 
                                 MapVote != paste(Map1, "to", Map1), 
                                 na.rm = TRUE),
                tie_wins = sum(Map1 == map & 
                             MapVote == paste(Map1, "to", Map1), 
                             na.rm = TRUE)
            )
        
        # Calculate win rate
        total_wins <- stats %>% 
            transmute(total = regular_wins + tie_wins) %>% 
            pull()
        
        win_rate <- round((total_wins / stats$appearances) * 100, 2)
        
        # Update results
        results[results$Map == map,] <- c(map, 
                                        stats$appearances, 
                                        stats$regular_wins,
                                        stats$tie_wins, 
                                        win_rate)
    }
    
    return(results)
}
```


```{r}
maxresults <- calculate_win_stats(combined)
```

### Visualization
```{r}
  # Optional: Remove rows with NA

ggplot(maxresults, aes(x = reorder(Map, Win_Rate), y = Win_Rate)) +
    geom_bar(stat = "identity", fill = "red") +
    coord_flip() +
    theme_minimal() +
    labs(title = "Map Win Rates in Voting",
         x = "Map",
         y = "Win Rate (%)") +
    theme(axis.text.y = element_text(size = 6), axis.text.x = element_text(size=4.5))
```

To answer research question we looked at the graph/chart for the following insights:

Nuketown '84 variants dominate with the highest win rates (Halloween version at 100%, regular version at 82%)
Crossroads Strike shows strong performance with a 77.6% win rate
Raid and Standoff are consistently popular choices, winning 75% and 70% of their appearances respectively

# Task 2 - Data Cleaning and Data Visualization with Generative AI Tool Selection {#part2}

Generative AI Tool Selection

For this task, we utilized ChatGPT (OpenAI GPT-4), known for its advanced natural language processing capabilities.

Prompts Used:
"we need to clean and analyze data from a video game voting dataset. The dataset includes information about two candidate maps (Map1 and Map2) and the final map chosen based on voting (Choice). Some rows include tie-breaking logic, where Map1 is selected by default in ties. our goal is to calculate the probability that each map wins when it is a candidate. Could you help me outline the data cleaning and analysis steps?"
"The dataset has inconsistencies, such as trailing spaces in map names, missing or NA values in Map1, Map2, and Choice, and potential misspellings of map names compared to a reference list in another file (CODMaps.csv)."
"To calculate the win probability for each map:
Count how often each map appears as a candidate (Map1 or Map2).
Count how often each map wins (Choice column).
Include tie-breaking logic where Map1 is selected in a tie. Could you provide R code to implement this logic and calculate win probabilities?"
"Once we calculate the win probabilities, we would like to visualize the results as a bar chart using ggplot2. Could you create R code to plot the maps on the x-axis and their win probabilities on the y-axis, with bars sorted by win probability?"

## Code Generated through AI
```{r}
library(dplyr)
library(ggplot2)
library(tidyr)

game_modes_df <- read.csv("CODGameModes.csv", stringsAsFactors = FALSE)
games_p1_df <- read.csv("CODGames_p1_380.csv", stringsAsFactors = FALSE)
games_p2_df <- read.csv("CODGames_p2_380.csv", stringsAsFactors = FALSE)
maps_df <- read.csv("CODMaps.csv", stringsAsFactors = FALSE)
```

```{r}
games_df <- bind_rows(games_p1_df, games_p2_df)
```

```{r}
# Clean column names and map names
games_df <- games_df %>%
  rename_with(trimws) %>%
  mutate(
    Map1 = trimws(Map1),
    Map2 = trimws(Map2),
    Choice = trimws(Choice)
  )

maps_df <- maps_df %>%
  rename_with(trimws) %>%
  mutate(Name = trimws(Name))

valid_maps <- maps_df$Name
```

```{r}
# Function to correct map names
correct_map_name <- function(map_name, valid_names) {
  if (is.na(map_name)) return(map_name)
  for (valid_name in valid_names) {
    if (tolower(map_name) == tolower(valid_name)) {
      return(valid_name)
    }
  }
  return(map_name)
}

```

```{r}
# Apply the correction function to Map1, Map2, and Choice
games_df <- games_df %>%
  mutate(
    Map1 = sapply(Map1, correct_map_name, valid_names = valid_maps),
    Map2 = sapply(Map2, correct_map_name, valid_names = valid_maps),
    Choice = sapply(Choice, correct_map_name, valid_names = valid_maps)
  )
```

```{r}
# Calculate the total occurrences of each map as a candidate
map_candidate_counts <- games_df %>%
  pivot_longer(cols = c(Map1, Map2), names_to = "Position", values_to = "Map") %>%
  filter(!is.na(Map)) %>%
  count(Map, name = "CandidateCount")
```

```{r}
# Calculate the total wins for each map in the Choice column
map_wins <- games_df %>%
  filter(!is.na(Choice)) %>%
  count(Choice, name = "WinCount")
```

```{r}
# Merge the counts and calculate win probabilities
map_stats <- map_candidate_counts %>%
  full_join(map_wins, by = c("Map" = "Choice")) %>%
  mutate(
    WinCount = replace_na(WinCount, 0),
    WinProbability = WinCount / CandidateCount
  )

```

```{r}
# Sort by WinProbability for visualization
map_stats <- map_stats %>%
  arrange(desc(WinProbability))
```

```{r}
# Visualization
ggplot(map_stats, aes(x = reorder(Map, -WinProbability), y = WinProbability)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(
    title = "Win Probability of Maps when Chosen as Candidates",
    x = "Map",
    y = "Win Probability"
  )

# Display results
print(map_stats)
```

Comparison: Generative AI vs. Manual Solution

Both our code and the generative AI solution effectively tackle the task of calculating map win rates, but their approaches have clear differences in methodology, strengths, and weaknesses. Both solutions clean the data to address issues like trailing spaces and missing values and use tidyverse and ggplot2 to produce bar plots that clearly display the win probabilities. While the outcomes are similar, the way each solution achieves these results is notably different.

Our code follows a methodical, step-by-step process. It carefully handles edge cases, such as correcting map names based on a reference dataset and applying tie-breaking logic when Map1 is selected by default. This thorough approach ensures reliability and accuracy, particularly when working with inconsistent or incomplete data. However, the iterative logic used in our code can make it slower when dealing with large datasets. The detailed nature of the code also means it’s slightly longer, but this makes it easier to understand and debug when needed.

The generative AI solution, on the other hand, focuses on speed and simplicity. By using vectorized operations, it processes data more efficiently, which is especially useful for larger datasets. Its compact and streamlined structure makes it quicker to write and implement. However, it relies on initial assumptions about data quality and consistency, which could result in errors when faced with more complex or messy datasets. While the AI-generated code produces polished visualizations and performs well for clean data, it does not address certain edge cases as comprehensively as our code.

In terms of their strengths, our code is highly reliable and accurate, making it better for datasets that require careful handling of inconsistencies. On the other hand, the AI-generated solution is faster and more efficient, excelling when the data is already clean and well-organized. Overall, our code is the better choice when reliability is a priority, especially for datasets with potential issues. However, the generative AI solution is more suitable when speed and efficiency are needed for large-scale data. Combining the thoroughness of our code with the speed of the AI solution would create an ideal balance between reliability and performance.

# Task 3: Inference {#part3}

## Relevant Information

**Research Question:** How does GameType affect TotalXP after accounting for the player's Score?

## Data Cleaning

```{r}
# Clean GameType to merge HC and non-HC variants
combined <- combined %>%
  mutate(GameType = str_remove(GameType, "HC - ")) %>%
  mutate(GameType = str_trim(GameType))
```

## Summary Statistics

Explore the distribution of TotalXP across GameTypes.

```{r}
combined %>%
  group_by(GameType) %>%
  summarize(
    MeanXP = mean(TotalXP, na.rm = TRUE),
    MedianXP = median(TotalXP, na.rm = TRUE),
    Count = n()
  ) %>%
  arrange(desc(MeanXP))
```

## Data Visualization

### Distribution of TotalXP by GameType

```{r}
ggplot(combined, aes(x = GameType, y = TotalXP)) +
  geom_boxplot() +
  theme_minimal() +
  labs(
    title = "Distribution of TotalXP by GameType",
    x = "GameType",
    y = "TotalXP"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### Scatter Plot of TotalXP vs. Score by GameType

```{r}
ggplot(combined, aes(x = Score, y = TotalXP, color = GameType)) +
  geom_point(alpha = 0.7) +
  theme_minimal() +
  labs(
    title = "TotalXP vs. Score by GameType",
    x = "Score",
    y = "TotalXP",
    color = "GameType"
  )
```

## Modeling

### Base Model: TotalXP ~ Score + GameType

```{r}
base_model <- lm(TotalXP ~ Score + GameType, data = combined)
summary(base_model)
```

### Improved Model: TotalXP ~ Score * GameType

```{r}

improved_model <- lm(TotalXP ~ Score * GameType, data = combined)
summary(improved_model)

# Ensure residuals and fitted values match the dataset size
combined <- combined %>%
  filter(!is.na(TotalXP) & !is.na(Score)) %>%
  mutate(
    residuals = residuals(improved_model),
    fitted = fitted(improved_model)
  )
```
##### Explanation for the improved model:
The improved model incorporates an interaction term (Score * GameType) to explore whether the relationship between Score and TotalXP varies across different GameTypes. This is based on the idea that some GameTypes may reward specific in-game actions or objectives differently, and these rewards could scale with Score in distinct ways. By including the interaction term, we can investigate how the effect of Score on TotalXP changes depending on the GameType.

## Model Diagnostics: Residual Analysis

#### Residuals vs Fitted Values

```{r}
ggplot(combined, aes(x = fitted, y = residuals)) +
  geom_point(alpha = 0.7) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  theme_minimal() +
  labs(
    title = "Residuals vs Fitted Values",
    x = "Fitted Values",
    y = "Residuals"
  )
```

#### QQ Plot for Residual Normality

```{r}
ggplot(combined, aes(sample = residuals)) +
  geom_qq() +
  geom_qq_line(color = "red") +
  theme_minimal() +
  labs(
    title = "QQ Plot of Residuals",
    x = "Theoretical Quantiles",
    y = "Sample Quantiles"
  )
```

## Results and Interpretation

The analysis shows that GameType significantly impacts TotalXP after accounting for the player's Score. GameTypes such as "Domination" yield the highest TotalXP, followed by "Hardpoint" and "TDM," reflecting the differences in objectives and gameplay dynamics. These results align with the expectation that modes requiring more strategic play and longer durations lead to higher TotalXP.

However, the models explain only about 30% of the variation in TotalXP, suggesting other factors, like player skill, time spent in-game, or specific in-game actions, also play a role. Including these variables could improve the model's accuracy. Additionally, the Score-TotalXP relationship may not be strictly linear, and testing non-linear models could provide a better fit.

The interaction terms in the improved model highlight that the effect of Score on TotalXP varies by GameType, but most were not statistically significant. Simplifying the model by focusing only on significant interactions or using advanced methods like random forests may capture these relationships more effectively.

Residual analysis indicates the model assumptions are mostly met, but refining the models with transformations or regression techniques could address issues with the dataset. 

# Task 4: Prediction {#part4}

For this task, we will be performing a comparative analysis of three different predictive models. We have opted to include Random Forest, Logistic Regression, and Naive Bayes models. The Random Forest model was required to be used, we were able to refer to the Logistic Regression model used within the notes, and we referred to the Naive Bayes model included in the caret library (found [here](https://www.geeksforgeeks.org/naive-bayes-classifier-in-r-programming/)) to compare how the probabilistic model compares with the other respective models. However, before we are able to properly begin performing the model creation process, we will need to develop our research question.       

### Research Question: How well can we predict the performance of the players team given the players individual metrics: Eliminations, Deaths, Score, and Damage?

Ultimately, we decided upon the variables: eliminations, Deaths, Score, and Damage for a couple of different reasons. Firstly, these are the statistical metrics of the players performance, and we have decided to include all except for the experience gain of the player as there are specific categories which create inconsistencies in the total experience gained in the match. Furthermore, if we decided to include only matches with a specific type of experience gain (XPType), then we will remove a significant amount of data from the model training which can impact the predictive capability of the different models. Therefore, we have decided to stick with the player performance metrics to predict the players team result. First, to implement the models we will need to perform some data cleaning to prepare the training data for the machine learning models.    

### Data Cleaning

```{r}
split <- function(X) {
  # Handle missing values
  
  if (is.na(X) || X == "") {
    return(c(NA, NA))
  }
  
  # Split the string
  parts <- unlist(strsplit(X, "-"))
  
  # Convert to numeric and return
  return(as.numeric(parts))
}
```

```{r}
# Apply the function to all of the results and store the results in a new data frame 
results <- sapply(X = combined$Result, FUN = split)

```

```{r}
# Store results as a data frame to create the Won variable
Results <- data.frame(
  Result = combined$Result,
  Player = results[1, ],
  Opposition = results[2, ]) %>%
  mutate(Won = ifelse(Player > Opposition, "Yes","No"))

combined$Won <- Results$Won

# Keep only the indices with no na's
combined <- combined[!is.na(combined$Won), ]
  
# Check the first 5 rows
combined %>%
  select(Result, Won) %>%
  head(5)
```

```{r}
# Create indicator for Won
combined <- 
  combined %>%
  mutate(Won_bit = ifelse(Won == "Yes", 1, 0))

# Check to make sure there are no na's
sum(is.na(combined$Won_bit))
```

```{r}
# Create Train and Test split
set.seed(123)
trainInd <- sample(1:nrow(combined), floor(0.8 * nrow(combined)))
set.seed(NULL)

Train <- combined[trainInd, ]
Validation <- combined[-trainInd, ]
```

### Model Creation

```{r}
set.seed(123)

# Random forest model
rfModel <- randomForest(as.factor(Won) ~ Eliminations+Deaths+Score+Damage, 
                        data = Train, 
                        ntree = 500, 
                        mtry = 3)

# Logistic regression model
logModel <- glm(Won_bit ~ Eliminations+Deaths+Score+Damage, family = binomial, data = Train)

# Naive Bayes Model
bayesModel <- naiveBayes(Won ~ Eliminations+Deaths+Score+Damage, data = Train)

set.seed(NULL)
```

### Model Exploration

```{r}
# For Random Forest
varImpPlot(rfModel, n.var = 6)
```

```{r}
# For Logistic Regression
summary(logModel)
```
#### Naive Bayes model was summarized in the next section


### Model Evaluation

```{r}
# Generate results for Random Forest Model
predWon <- predict(rfModel, newdata = Validation)
  
#Create confusion matrix
table(predWon, Validation$Won)

#Calculate accuracy
mean(predWon == Validation$Won)
```

```{r}
# Generate results for Logistic Regression Model

# Establish threshold
threshold <- 0.50

# Generate predictions
pred_prob <- predict(logModel, newdata = Validation, type = "response")

#  
pred_won <- ifelse(pred_prob > threshold, "Yes", "No")

#Create confusion matrix
table(pred_won, Validation$Won)

#Calculate accuracy
mean(pred_won == Validation$Won)
```

```{r}
# Generate results for Naive Bayes Model
y_pred <- predict(bayesModel, newdata = Validation)
cm <- table(Validation$Won, y_pred)
confusionMatrix(cm)
```

### Analysis of Models

Overall, the models performed with a very similar level of prediction accuracy with the Logistic Regression model with a threshold of 0.5 performing the best with an overall accuracy over 0.7. We decided to use a threshold for prediction probability of over 0.5 as the threshold so that we could divide the values in half, as we did not want the threshold to be a large determiner in the overall accuracy of the model predictions. The next highest performing model was the Random Forest model with an accuracy score of 0.679. Finally, the Naive Bayes model came in just behind with an overall accuracy of 0.672. It is pertinent to note that all of these scores were the result of the model predicting new data that we had originally separated when training the model.    

To make a more comprehensive analysis of the different models, we have opted to compare the confusion matrices that I have generated for each of the respective models. From the analysis, you can see that the Logistic Regression model predicted the most number of TP with 73, while having the lowest number of FP with only 16. Meanwhile, the Random Forest Model predicted the negative values with the most success, with 46 TN and only 27 FN. 

While it is interesting to compare the different models, it is likely that for the purposes of predicting the team result relative to the players overall performance, roughly 70% accuracy from the different classification model applications is a fairly reasonable result. We can see this conceptually in the feature importance visual for the random forest and the summary of the logistic regression model. The models contain completely opposite feature importance hierarchy as determined by the decrease in gini gain and the coefficients respectively. As the random forest model obtains the largest gini gain from the Score and Damage variables, the Logistic Model uses the Eliminations and Deaths attributes to determine the team result. Overall, it is interesting to compare the different prediction metrics although our answer for our question ultimately lies around a predictive accuracy of 70% for the team result based on the individual player match performance attributes.       


